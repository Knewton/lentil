{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lentil import datatools\n",
    "from lentil import models\n",
    "from lentil import est\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_path = os.path.join('data', 'assistments_2009_2010.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(history_path, 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = history.data\n",
    "duration = history.duration()\n",
    "\n",
    "num_students = history.num_students()\n",
    "num_left_out_students = int(0.3 * num_students)\n",
    "left_out_student_ids = {history.id_of_student_idx(\n",
    "        student_idx) for student_idx in random.sample(\n",
    "        range(num_students), num_left_out_students)}\n",
    "\n",
    "left_out_ixns = df['student_id'].isin(left_out_student_ids)\n",
    "left_in_ixns = ~left_out_ixns\n",
    "left_in_modules = set(df[left_in_ixns]['module_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of unique students = %d\" % (len(df['student_id'].unique()))\n",
    "print \"Number of unique modules = %d\" % (len(df['module_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect bubble scenarios from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# these constraints speed up bubble collection\n",
    "MIN_BUBBLE_LENGTH = 2 # number of interactions\n",
    "MAX_BUBBLE_LENGTH = 20 # number of interactions\n",
    "\n",
    "grouped = history.data.groupby('student_id')\n",
    "\n",
    "# dict[(str, str), dict[tuple, int]]\n",
    "# (start lesson id, final assessment id) -> \n",
    "# dict[(lesson id, lesson id, ...) -> \n",
    "# number of students who took this path]\n",
    "bubble_paths = {}\n",
    "\n",
    "# dict[(str, str, (str, str, ...)), list[str]]\n",
    "# (start lesson id, final assessment id, lesson sequence) -> \n",
    "# [ids of students who took this path]\n",
    "bubble_students = defaultdict(list)\n",
    "\n",
    "# dict[(str, str, (str, str, ...)), set[str]]\n",
    "# (start lesson id, final assessment id, lesson sequence) -> \n",
    "# [outcomes for students who took this path]\n",
    "bubble_outcomes = defaultdict(list)\n",
    "\n",
    "for student_id in left_out_student_ids:\n",
    "    group = grouped.get_group(student_id)\n",
    "    module_ids = list(group['module_id'])\n",
    "    module_types = list(group['module_type'])\n",
    "    outcomes = list(group['outcome'])\n",
    "    for i, start_lesson_id in enumerate(module_ids):\n",
    "        if module_types[i]!=datatools.LessonInteraction.MODULETYPE:\n",
    "            continue\n",
    "        if start_lesson_id not in left_in_modules:\n",
    "            continue\n",
    "        for j, (final_assessment_id, module_type, outcome) in enumerate(zip(module_ids[(i+MIN_BUBBLE_LENGTH):(i+MAX_BUBBLE_LENGTH)], \n",
    "                                                            module_types[(i+MIN_BUBBLE_LENGTH):(i+MAX_BUBBLE_LENGTH)], \n",
    "                                                            outcomes[(i+MIN_BUBBLE_LENGTH):(i+MAX_BUBBLE_LENGTH)])):\n",
    "            if final_assessment_id not in left_in_modules:\n",
    "                break\n",
    "            if module_type==datatools.AssessmentInteraction.MODULETYPE:\n",
    "                lesson_seq = [x for x, y in zip(module_ids[i:(i+MIN_BUBBLE_LENGTH+j)],\n",
    "                                                    module_types[i:(i+MIN_BUBBLE_LENGTH+j)]) if y==datatools.LessonInteraction.MODULETYPE]\n",
    "                path = tuple(lesson_seq)\n",
    "                if any(m not in left_in_modules for m in path):\n",
    "                    break\n",
    "                                    \n",
    "                try:\n",
    "                    bubble_paths[(start_lesson_id, final_assessment_id)][path] += 1\n",
    "                except KeyError:\n",
    "                    bubble_paths[(start_lesson_id, final_assessment_id)] = defaultdict(int)\n",
    "                    bubble_paths[(start_lesson_id, final_assessment_id)][path] += 1\n",
    "                    \n",
    "                bubble_students[(start_lesson_id, final_assessment_id, path)].append(student_id)\n",
    "                bubble_outcomes[(start_lesson_id, final_assessment_id, path)].append(1 if outcome else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_NUM_STUDENTS_ON_PATH = 10\n",
    "\n",
    "# dict[(str, str, (str, str, ...), (str, str, ...)]\n",
    "# (start lesson id, final assessment id, path, other_path) -> \n",
    "# ([ids of students who took path], [ids of students who took other_path])\n",
    "my_bubble_students = {}\n",
    "\n",
    "# dict[(str, str, (str, str, ...), (str, str, ...)]\n",
    "# (start lesson id, final assessment id, path, other_path) -> \n",
    "# ([outcomes for students who took path], [outcomes for students who took other_path])\n",
    "my_bubble_outcomes = {}\n",
    "\n",
    "for (start_lesson_id, final_assessment_id), d in bubble_paths.iteritems():\n",
    "    paths = [path for path, num_students_on_path in d.iteritems() if num_students_on_path>=MIN_NUM_STUDENTS_ON_PATH]\n",
    "    for i, path in enumerate(paths):\n",
    "        for other_path in paths[(i+1):]:\n",
    "            if len(path) != len(other_path):\n",
    "                # paths must have the same number of lesson interactions\n",
    "                # in order to be part of a bubble\n",
    "                continue\n",
    "            my_bubble_students[(start_lesson_id, final_assessment_id, path, other_path)] = (bubble_students[(start_lesson_id, final_assessment_id, path)], bubble_students[(start_lesson_id, final_assessment_id, other_path)])\n",
    "            my_bubble_outcomes[(start_lesson_id, final_assessment_id, path, other_path)] = (bubble_outcomes[(start_lesson_id, final_assessment_id, path)], bubble_outcomes[(start_lesson_id, final_assessment_id, other_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bubble_paths = None # clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_NUM_STUDENTS_IN_BUBBLE = 10 # minimum num students on a branch\n",
    "MIN_AVG_BUBBLE_PATH_LENGTH = 2 # minimum branch length\n",
    "\n",
    "def is_valid_bubble(k):\n",
    "    students_on_path, students_on_other_path = my_bubble_students[k]\n",
    "    pass_rate = np.mean(sum(my_bubble_outcomes[k], []))\n",
    "    num_students_in_bubble = min(len(students_on_path), len(students_on_other_path))\n",
    "    path_length = min(len(path), len(other_path))\n",
    "    return pass_rate!=0 and pass_rate!=1 and num_students_in_bubble >= MIN_NUM_STUDENTS_IN_BUBBLE and path_length >= MIN_AVG_BUBBLE_PATH_LENGTH\n",
    "\n",
    "is_valid_bubble_memo = {k: is_valid_bubble(k) for k in my_bubble_students}\n",
    "print \"Number of valid bubbles = %d\" % sum(1 for v in is_valid_bubble_memo.itervalues() if v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out invalid bubbles\n",
    "ks = set()\n",
    "for (mid, later_mid, path, other_path), is_valid in is_valid_bubble_memo.iteritems():\n",
    "    if is_valid:\n",
    "        ks |= {(mid, later_mid, path), (mid, later_mid, other_path)}\n",
    "\n",
    "for k, is_valid in is_valid_bubble_memo.iteritems():\n",
    "    mid, later_mid, path, other_path = k\n",
    "    if not is_valid:\n",
    "        my_bubble_students.pop(k, None)\n",
    "        my_bubble_outcomes.pop(k, None)\n",
    "        if (mid, later_mid, path) not in ks:\n",
    "            bubble_students.pop((mid, later_mid, path), None)\n",
    "            bubble_outcomes.pop((mid, later_mid, path), None)\n",
    "        if (mid, later_mid, other_path) not in ks:\n",
    "            bubble_students.pop((mid, later_mid, other_path), None)\n",
    "            bubble_outcomes.pop((mid, later_mid, other_path), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Size of bubble experiment (number of students in smaller arm)')\n",
    "plt.ylabel('Frequency (number of bubbles)')\n",
    "plt.hist([min(len(v[0]), len(v[1])) for k, v in my_bubble_students.iteritems()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Size of bubble experiment (number of students in smaller arm)')\n",
    "plt.ylabel('Frequency (number of bubbles)')\n",
    "plt.hist([len(v[0]) + len(v[1]) for k, v in my_bubble_students.iteritems()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Bubble path length (number of lesson interactions in shorter arm)')\n",
    "plt.ylabel('Frequency (number of bubbles)')\n",
    "plt.hist([min(len(path), len(other_path)) for _, _, path, other_path in my_bubble_students])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Bubble path length (number of lesson interactions in both arms)')\n",
    "plt.ylabel('Frequency (number of bubbles)')\n",
    "plt.hist([len(path) + len(other_path) for _, _, path, other_path in my_bubble_students])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Size of bubble experiment (number of students)')\n",
    "plt.ylabel('Average bubble path length (number of lesson interactions)')\n",
    "plt.scatter([min(len(v[0]), len(v[1])) for k, v in my_bubble_students.iteritems()],\n",
    "            [min(len(path), len(other_path)) for _, _, path, other_path in my_bubble_students],\n",
    "            alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform bubble experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instead of embedding each bubble separately \n",
    "# (which would take a long time),\n",
    "# assign bubbles to embedding \"rounds\" so that multiple \n",
    "# bubbles can be embedded simultaneously.\n",
    "# in practice, the sets of students involved in different bubbles\n",
    "# are disjoint enough that the resulting number of rounds\n",
    "# is much lower than the number of bubbles.\n",
    "unassigned_bubbles = set(my_bubble_students.keys())\n",
    "rounds = []\n",
    "while len(unassigned_bubbles)>0:\n",
    "    used = set()\n",
    "    rd = []\n",
    "    \n",
    "    for bubble in unassigned_bubbles:\n",
    "        students = set(sum(my_bubble_students[bubble], []))\n",
    "        if len(students - used) < len(students):\n",
    "            continue\n",
    "        used |= students\n",
    "        rd.append(bubble)\n",
    "    \n",
    "    rounds.append(rd)\n",
    "    unassigned_bubbles -= set(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of bubbles covered = %d\" % sum([len(r) for r in rounds])\n",
    "print \"Number of rounds = %d\" % len(rounds)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Number of bubbles in round')\n",
    "plt.plot([len(r) for r in rounds])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for \"playing\" lesson interactions over students\n",
    "# and computing the expected student embedding at the end of the lessons\n",
    "def play_lessons_over_student(initial_student_embedding, lesson_and_prereq_embeddings):\n",
    "    \"\"\"\n",
    "    Simulate a student completing a sequence of lesson interactions,\n",
    "    where gains are modulated by prereqs\n",
    "    \n",
    "    :param np.ndarray initial_student_embedding: The embedding of the student\n",
    "        before completing any lesson interactions\n",
    "    \n",
    "    :param list[(np.ndarray, np.ndarray)] lesson_and_prereq_embeddings:\n",
    "        A list of tuples (lesson embedding, prereq embedding) that contains\n",
    "        the sequence of lessons the student will complete\n",
    "        \n",
    "    :rtype: np.ndarray\n",
    "    :return: The expected student embedding at the end of the lesson sequence\n",
    "    \"\"\"\n",
    "    return reduce(\n",
    "        lambda prev_student_embedding, (lesson_embedding, prereq_embedding): prev_student_embedding + lesson_embedding / (1 + math.exp(\n",
    "                -(np.dot(prev_student_embedding, prereq_embedding) / np.linalg.norm(prereq_embedding) - np.linalg.norm(prereq_embedding)))), \n",
    "        lesson_and_prereq_embeddings, initial_student_embedding)\n",
    "\n",
    "def play_lessons_over_student_without_prereqs(initial_student_embedding, lesson_embeddings):\n",
    "    \"\"\"\n",
    "    Simulate a student completing a sequence of lesson interactions,\n",
    "    where gains are not modulated by prereqs\n",
    "    \n",
    "    :param np.ndarray initial_student_embedding: The embedding of the student\n",
    "        before completing any lesson interactions\n",
    "    \n",
    "    :param list[np.ndarray] lesson_embeddings:\n",
    "        A list of embeddings for the lessons that the student will complete\n",
    "        \n",
    "    :rtype: np.ndarray\n",
    "    :return: The expected student embedding at the end of the lesson sequence\n",
    "    \"\"\"\n",
    "    return reduce(\n",
    "        lambda prev_student_embedding, lesson_embedding: prev_student_embedding + lesson_embedding, \n",
    "        lesson_embeddings, initial_student_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding(\n",
    "    embedding_kwargs,\n",
    "    estimator,\n",
    "    history,\n",
    "    filtered_history,\n",
    "    split_history=None):\n",
    "    \n",
    "    model = models.EmbeddingModel(history, **embedding_kwargs)\n",
    "    \n",
    "    estimator.filtered_history = filtered_history\n",
    "    if split_history is not None:\n",
    "        estimator.split_history = split_history\n",
    "    \n",
    "    model.fit(estimator)\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimator = est.EmbeddingMAPEstimator(\n",
    "    regularization_constant=1e-6,\n",
    "    using_scipy=True,\n",
    "    verify_gradient=False,\n",
    "    debug_mode_on=True,\n",
    "    ftol=1e-3)\n",
    "\n",
    "def meta_meta_build_embedding(embedding_kwargs):\n",
    "    def meta_build_embedding(\n",
    "        history,\n",
    "        filtered_history,\n",
    "        split_history=None):\n",
    "\n",
    "        return build_embedding(\n",
    "            embedding_kwargs,\n",
    "            estimator,\n",
    "            history,\n",
    "            filtered_history,\n",
    "            split_history=split_history)\n",
    "    \n",
    "    return meta_build_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_kwargs = {\n",
    "    'embedding_dimension' : 2,\n",
    "    'using_lessons' : True,\n",
    "    'using_prereqs' : True,\n",
    "    'using_bias' : True,\n",
    "    'learning_update_variance_constant' : 0.5\n",
    "}\n",
    "\n",
    "model_builders = {\n",
    "    'd=2, with prereqs and bias' : meta_meta_build_embedding(embedding_kwargs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_embedding(\n",
    "    model,\n",
    "    history,\n",
    "    lesson_seqs,\n",
    "    timestep_of_bubble_start,\n",
    "    final_assessment_id):\n",
    "    \"\"\"\n",
    "    For students in the bubble, compute the expected pass likelihood on\n",
    "    the final assessment if they take their recommended path or the alternative path\n",
    "    \n",
    "    :param models.EmbeddingModel model: A trained embedding\n",
    "    :param datatools.InteractionHistory history: History used to train the model\n",
    "    :param dict[str, (str, str, ...)] lesson_seqs:\n",
    "        A dictionary mapping student_id to the lesson sequence of the path\n",
    "        they actually took\n",
    "        \n",
    "    :param dict[str, int] timestep_of_bubble_start:\n",
    "        A dictionary mapping student_id to the timestep at which that student\n",
    "        worked on the start lesson for the bubble\n",
    "        \n",
    "    :param str final_assessment_id: The id of the assessment at the end of the lesson\n",
    "    :param list[str] students: A list of ids for the students participating in the bubble\n",
    "    :rtype: (np.array, np.array)\n",
    "    :return: A tuple of (pass likelihoods )\n",
    "    \"\"\"\n",
    "    \n",
    "    lesson_seqs_vals = list(set(lesson_seqs.values()))\n",
    "    \n",
    "    if model.using_prereqs:\n",
    "        get_lesson_seq = {v:[(\n",
    "                    model.lesson_embeddings[history.idx_of_lesson_id(lesson_id), :], \n",
    "                    model.prereq_embeddings[history.idx_of_lesson_id(lesson_id), :]) for lesson_id in v] for v in lesson_seqs_vals}\n",
    "    else:\n",
    "        get_lesson_seq = {v:[model.lesson_embeddings[history.idx_of_lesson_id(lesson_id), :] for lesson_id in v] for v in lesson_seqs_vals}\n",
    "\n",
    "    lesson_seqs_taken = {student_id:get_lesson_seq[lesson_seqs[student_id]] for student_id in lesson_seqs}\n",
    "    \n",
    "    lesson_seqs_v = {k: i for i, k in enumerate(lesson_seqs_vals)}\n",
    "    lesson_seqs_nontaken = {student_id:get_lesson_seq[lesson_seqs_vals[(lesson_seqs_v[lesson_seqs[student_id]]+1)%2]] for student_id in lesson_seqs}\n",
    "        \n",
    "    lesson_simulator = play_lessons_over_student if model.using_prereqs else play_lessons_over_student_without_prereqs\n",
    "        \n",
    "    expected_students_after_taken_path = {student_id:lesson_simulator(\n",
    "            model.student_embeddings[history.idx_of_student_id(student_id), :, \n",
    "                                     timestep_of_bubble_start[student_id]], seq) for student_id, seq in lesson_seqs_taken.iteritems()}\n",
    "    expected_students_after_nontaken_path = {student_id:lesson_simulator(\n",
    "            model.student_embeddings[history.idx_of_student_id(student_id), :, \n",
    "                                     timestep_of_bubble_start[student_id]], seq) for student_id, seq in lesson_seqs_nontaken.iteritems()}\n",
    "\n",
    "    assessment_idx = history.idx_of_assessment_id(final_assessment_id)\n",
    "    assessment_embedding = model.assessment_embeddings[assessment_idx, :]\n",
    "    assessment_bias = model.assessment_biases[assessment_idx]\n",
    "        \n",
    "    pass_likelihoods_after_taken_path = {student_id:math.exp(\n",
    "            model.assessment_outcome_log_likelihood_helper(\n",
    "                expected_student_embedding,\n",
    "                assessment_embedding,\n",
    "                model.student_biases[history.idx_of_student_id(student_id)],\n",
    "                assessment_bias,\n",
    "                True)) for student_id, expected_student_embedding in expected_students_after_taken_path.iteritems()}\n",
    "    \n",
    "    pass_likelihoods_after_nontaken_path = {student_id:math.exp(\n",
    "            model.assessment_outcome_log_likelihood_helper(\n",
    "                expected_student_embedding,\n",
    "                assessment_embedding,\n",
    "                model.student_biases[history.idx_of_student_id(student_id)],\n",
    "                assessment_bias,\n",
    "                True)) for student_id, expected_student_embedding in expected_students_after_nontaken_path.iteritems()}\n",
    "    \n",
    "    return pass_likelihoods_after_taken_path, pass_likelihoods_after_nontaken_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_evals = {\n",
    "    'd=2, with prereqs and bias' : eval_embedding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rounds = len(rounds)\n",
    "timestep_of_last_interaction = df.groupby('student_id')['timestep'].max()\n",
    "\n",
    "grouped_by_module = df.groupby('module_id')\n",
    "# timestep_of_bubble_start[round][student_id]\n",
    "# = timestep of lesson interaction at bubble start\n",
    "timestep_of_bubble_start = [{} for _ in xrange(num_rounds)]\n",
    "for i, rd in enumerate(rounds):\n",
    "    for (start_lesson_id, final_assessment_id, path, other_path) in rd:\n",
    "        students = sum(my_bubble_students[(start_lesson_id, final_assessment_id, path, other_path)], [])\n",
    "        grouped_by_student = grouped_by_module.get_group(start_lesson_id).groupby('student_id')\n",
    "        for student_id in students:\n",
    "            student_group = grouped_by_student.get_group(student_id)\n",
    "            timestep_of_bubble_start[i][student_id] = list(student_group['timestep'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "student_pass_likelihoods = [[{k: None for k in model_builders} for _ in rd] for rd in rounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, rd in enumerate(rounds):\n",
    "    print '%d of %d' % (i, num_rounds)\n",
    "    \n",
    "    t = df['student_id'].apply(\n",
    "        lambda x: timestep_of_bubble_start[i][x] if x in timestep_of_bubble_start[i] else timestep_of_last_interaction.ix[x])\n",
    "    \n",
    "    filtered_history = df[(left_in_ixns) | ((left_out_ixns) & df['timestep']<=t)]\n",
    "    \n",
    "    split_history = history.split_interactions_by_type(\n",
    "            filtered_history=filtered_history)\n",
    "    \n",
    "    round_models = {}\n",
    "    for k, build_model in model_builders.iteritems():\n",
    "        round_models[k] = build_model(\n",
    "            history,\n",
    "            filtered_history,\n",
    "            split_history=split_history)\n",
    "            \n",
    "    for j, (start_lesson_id, final_assessment_id, path, other_path) in enumerate(rd):\n",
    "        students_on_path, students_on_other_path = my_bubble_students[(start_lesson_id, final_assessment_id, path, other_path)]\n",
    "        lesson_seqs = {student_id: path for student_id in students_on_path}\n",
    "        lesson_seqs.update({student_id: other_path for student_id in students_on_other_path})\n",
    "        for k, eval_model in model_evals.iteritems():\n",
    "            student_pass_likelihoods[i][j][k] = eval_model(\n",
    "                round_models[k],\n",
    "                history,\n",
    "                lesson_seqs,\n",
    "                timestep_of_bubble_start[i],\n",
    "                final_assessment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct feature space for students\n",
    "\n",
    "# if we plan to use PCA to map students to a lower-dimensional feature space\n",
    "# we really should construct features for all students (not just left-out students).\n",
    "# in practice, there are so many students that PCA runs quite slow,\n",
    "# so we add a large number of \"left-in\" students instead of all of them.\n",
    "NUM_EXTRA_STUDENTS = 0\n",
    "\n",
    "num_assessments = history.num_assessments()\n",
    "num_modules = num_assessments + history.num_lessons()\n",
    "idx_of_module_id = {module_id: idx for idx, module_id in enumerate(history.iter_assessments())}\n",
    "for idx, module_id in enumerate(history.iter_lessons()):\n",
    "    idx_of_module_id[module_id] = num_assessments + idx\n",
    "    \n",
    "students_in_bubbles = {student_id for v in my_bubble_students.itervalues() for student_id in sum(v, [])}\n",
    "print \"Number of unique students in bubbles = %d\" % len(students_in_bubbles)\n",
    "students_in_bubbles |= set(random.sample(history.data['student_id'].unique(), NUM_EXTRA_STUDENTS))\n",
    "students_in_bubbles = {k: i for i, k in enumerate(students_in_bubbles)}\n",
    "\n",
    "grouped = df.groupby('student_id')\n",
    "X = np.zeros((len(students_in_bubbles), num_modules))\n",
    "for student_id, student_idx in students_in_bubbles.iteritems():\n",
    "    group = grouped.get_group(student_id)\n",
    "    for module_id, outcome in zip(group['module_id'], group['outcome']):\n",
    "        X[student_idx, idx_of_module_id[module_id]] = 1 if outcome is None else (1 if outcome else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map students to low-dimensional feature space using PCA\n",
    "NUM_COVARIATES = 1000\n",
    "\n",
    "pca = PCA(n_components=NUM_COVARIATES)\n",
    "XS = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(pca.explained_variance_ratio_)\n",
    "x = [None] * N\n",
    "x[0] = pca.explained_variance_ratio_[0]\n",
    "y = range(N)\n",
    "for i in xrange(1, N):\n",
    "    x[i] = x[i-1] + pca.explained_variance_ratio_[i]\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.plot(y, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of folds in k-fold cross-validation used to select\n",
    "# an L2-regularization constant for logistic regression\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# regularization constants to select from\n",
    "Cs = [1e-3, 1e-2, 0.1, 1.0, 10, 100]\n",
    "\n",
    "propensity_scores = {k: {} for k in model_builders}\n",
    "\n",
    "for model in model_builders:\n",
    "    for i, rd in enumerate(rounds):\n",
    "        for j, (bubble_pass_likelihoods, (start_lesson_id, final_assessment_id, path, other_path)) in enumerate(zip(student_pass_likelihoods[i], rd)):\n",
    "            students = sum(my_bubble_students[(start_lesson_id, final_assessment_id, path, other_path)], [])\n",
    "            student_idxes = np.array([students_in_bubbles[student_id] for student_id in students])\n",
    "            myX = X[student_idxes, :]\n",
    "\n",
    "            pass_likelihoods_on_taken_path, pass_likelihoods_on_nontaken_path = bubble_pass_likelihoods[model]\n",
    "            Y = np.array([1 if pass_likelihoods_on_taken_path[student_id] >= pass_likelihoods_on_nontaken_path[student_id] else 0 for student_id in students])\n",
    "            \n",
    "            if len(set(Y)) <= 1:\n",
    "                # not enough students took their recommended path\n",
    "                continue\n",
    "                \n",
    "            # select L2-regularization constant using cross-validation  \n",
    "            kf = KFold(\n",
    "                len(students),\n",
    "                n_folds=NUM_FOLDS,\n",
    "                shuffle=True)\n",
    "\n",
    "            val_lls = [[] for _ in xrange(len(Cs))] # average log-likelihoods\n",
    "            for k, (train_idxes, val_idxes) in enumerate(kf):\n",
    "                def compute_val_ll(lreg_model):\n",
    "                    \"\"\"\n",
    "                    Compute average log-likelihood of validation student participation\n",
    "                    \"\"\"\n",
    "                    log_probas = [lreg_model.predict_log_proba(x) for x in myX[val_idxes]]\n",
    "                    idx_of_zero = 0 if lreg_model.classes_[0]==0 else 1\n",
    "                    return np.mean([ll[0, (y_true ^ idx_of_zero)] for ll, y_true in zip(log_probas, Y[val_idxes])])\n",
    "\n",
    "                for i, C in enumerate(Cs):\n",
    "                    if len(set(Y[train_idxes])) == 1:\n",
    "                        continue\n",
    "                    lreg_model = LogisticRegression(penalty='l2', C=C)\n",
    "                    lreg_model.fit(myX[train_idxes], Y[train_idxes])\n",
    "                    val_lls[i].append(compute_val_ll(lreg_model))\n",
    "\n",
    "            # select C that gives the highest validation average log-likelihood\n",
    "            C = Cs[max(range(len(Cs)), key=lambda i: np.mean(val_lls[i]))]\n",
    "\n",
    "            lreg_model = LogisticRegression(penalty='l2', C=C)\n",
    "            lreg_model.fit(myX, Y)\n",
    "\n",
    "            idx_of_one = 0 if lreg_model.classes_[0]==1 else 1\n",
    "\n",
    "            propensity_scores[model][(start_lesson_id, final_assessment_id, path, other_path)] = {student_id:lreg_model.predict_proba(\n",
    "                                X[students_in_bubbles[student_id],:])[0, idx_of_one] for student_id in students}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some bubbles probably got filtered\n",
    "# for having too few students who took their recommended path\n",
    "for k, v in propensity_scores.iteritems():\n",
    "    print \"%s\\nNumber of bubbles with propensity scores = %d\\n\" % (k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def performance_vs_path_quality_diff(\n",
    "    model,\n",
    "    compute_performance_metric,\n",
    "    list_num_neighbors_to_match_on,\n",
    "    threshold_tick_size=0.05,\n",
    "    min_threshold=0.,\n",
    "    max_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Generic function for computing a performance metric using\n",
    "    different nearest neighbor matching and conditioning on bubbles\n",
    "    \n",
    "    :param int matching: \n",
    "        0 => no propensity score matching\n",
    "        k => k-nearest neighbor matching\n",
    "        \n",
    "    :param float threshold_tick_size: Tick size for threshold on path quality difference\n",
    "    :param float min_threshold: Minimum threshold for path quality difference\n",
    "    :param float max_threshold: Maximum threshold for path quality difference\n",
    "    :param function compute_metric:\n",
    "        Compute performance on a single bubble::\n",
    "            \n",
    "            pass rate on recommended path, pass rate on non-recommended path -> metric\n",
    "        \n",
    "    :rtype: (list[list[float]], list[list[float]], list[float])\n",
    "    :return: \n",
    "        Performance metrics and standard errors, \n",
    "        for each number of nearest neighbors to match on, \n",
    "        for each threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    thresholds = np.arange(min_threshold, max_threshold, threshold_tick_size)\n",
    "    performance_metrics = [[] for _ in list_num_neighbors_to_match_on]\n",
    "    performance_metric_stderrs = [[] for _ in list_num_neighbors_to_match_on]\n",
    "\n",
    "    for n, num_neighbors_to_match_on in enumerate(list_num_neighbors_to_match_on):\n",
    "        for threshold in thresholds:\n",
    "            performance_metrics_for_threshold = []\n",
    "            # iterate over rounds\n",
    "            for i, rd in enumerate(rounds):\n",
    "                # iterate over bubbles in round\n",
    "                for j, k in enumerate(rd):\n",
    "                    start_lesson_id, final_assessment_id, path, other_path = k\n",
    "                    path_students, other_path_students = my_bubble_students[k]\n",
    "\n",
    "                    path_outcomes, other_path_outcomes = my_bubble_outcomes[k]\n",
    "                    path_outcomes = {student_id: outcome for student_id, outcome in zip(path_students, path_outcomes)}\n",
    "                    other_path_outcomes = {student_id: outcome for student_id, outcome in zip(other_path_students, other_path_outcomes)}\n",
    "                    outcomes = {}\n",
    "                    outcomes.update(path_outcomes)\n",
    "                    outcomes.update(other_path_outcomes)\n",
    "\n",
    "                    path_pass_rate = np.mean(path_outcomes.values())\n",
    "                    other_path_pass_rate = np.mean(other_path_outcomes.values())  \n",
    "\n",
    "                    if abs(path_pass_rate - other_path_pass_rate) < threshold:\n",
    "                        continue\n",
    "\n",
    "                    # propensity score matching\n",
    "                    if num_neighbors_to_match_on > 0:\n",
    "                        # try to get propensity scores\n",
    "                        try:\n",
    "                            prop_scores = propensity_scores[model][k]\n",
    "                        except KeyError:\n",
    "                            # propensity scores not available for this bubble\n",
    "                            continue\n",
    "\n",
    "                        larger_student_group, smaller_student_group = (path_students, other_path_students) if len(path_students) > len(other_path_students) else (other_path_students, path_students)\n",
    "\n",
    "                        matched_students = set()\n",
    "                        for student_id in larger_student_group:\n",
    "                            ps = prop_scores[student_id]\n",
    "                            matched_students |= set(sorted(\n",
    "                                    smaller_student_group, \n",
    "                                    key=lambda other_student_id: abs(\n",
    "                                        prop_scores[other_student_id] - ps))[:num_neighbors_to_match_on])\n",
    "\n",
    "                        matched_students |= set(larger_student_group)\n",
    "                    else:\n",
    "                        matched_students = set(path_students) | set(other_path_students)\n",
    "\n",
    "                    outcomes = {student_id: v for student_id, v in outcomes.iteritems() if student_id in matched_students}\n",
    "\n",
    "                    pass_likelihoods_on_taken_path, pass_likelihoods_on_nontaken_path = student_pass_likelihoods[i][j][model]\n",
    "                    outcomes_on_recommended_path = [1 if outcome else 0 for student_id, outcome in outcomes.iteritems() if pass_likelihoods_on_taken_path[student_id] >= pass_likelihoods_on_nontaken_path[student_id]]\n",
    "                    outcomes_on_nonrecommended_path = [1 if outcome else 0 for student_id, outcome in outcomes.iteritems() if pass_likelihoods_on_taken_path[student_id] < pass_likelihoods_on_nontaken_path[student_id]]\n",
    "                    if outcomes_on_recommended_path == [] or outcomes_on_nonrecommended_path == []:\n",
    "                        # nobody took the recommended path, or nobody took the non-recommended path\n",
    "                        continue\n",
    "                    pass_rate_on_recommended_path = np.mean(outcomes_on_recommended_path)\n",
    "                    pass_rate_on_nonrecommended_path = np.mean(outcomes_on_nonrecommended_path)\n",
    "\n",
    "                    pm = compute_performance_metric(pass_rate_on_recommended_path, pass_rate_on_nonrecommended_path)\n",
    "                    if not np.isinf(pm):\n",
    "                        performance_metrics_for_threshold.append(pm)\n",
    "\n",
    "            performance_metrics[n].append(np.mean(performance_metrics_for_threshold))\n",
    "            performance_metric_stderrs[n].append(np.std(performance_metrics_for_threshold) / math.sqrt(len(performance_metrics_for_threshold)))\n",
    "    return performance_metrics, performance_metric_stderrs, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_performance_vs_path_quality_diff(\n",
    "    model, metrics, stderrs, thresholds, labels,\n",
    "    metric_name='Performance',\n",
    "    random_baseline=0):\n",
    "    \n",
    "    for m, l, s in zip(metrics, labels, stderrs):\n",
    "        plt.plot(thresholds, m, label=l, linewidth=3)\n",
    "        plt.errorbar(thresholds, m, yerr=1.96*np.array(s), color='black')\n",
    "\n",
    "    plt.plot(thresholds, [random_baseline] * len(thresholds),\n",
    "             '--',\n",
    "             color='black',\n",
    "             label='random')\n",
    "\n",
    "    plt.xlabel('Minimum difference in path quality')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(model)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.legend(bbox_to_anchor=(1., 1.))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = 'd=2, with prereqs and bias'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neighbors_to_match_on = [0, 1, 3, 5]\n",
    "labels = ['no matching'] + ['%d-NN matching' % (x) for x in num_neighbors_to_match_on[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_relative_gain = lambda p, q: p/(1-p)*(1-q)/q\n",
    "metric_name = 'Expected relative gain from recommended path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance_metrics, performance_metric_stderrs, thresholds = performance_vs_path_quality_diff(\n",
    "    model, \n",
    "    compute_relative_gain,\n",
    "    num_neighbors_to_match_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_performance_vs_path_quality_diff(\n",
    "    model,\n",
    "    performance_metrics, performance_metric_stderrs, thresholds, labels,\n",
    "    metric_name=metric_name,\n",
    "    random_baseline=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is similarity in path composition related to difference in path quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_sim, quality_diff = [], []\n",
    "for i, rd in enumerate(rounds):\n",
    "    for j, (start_lesson_id, final_assessment_id, path, other_path) in enumerate(rd):\n",
    "        path_outcomes, other_path_outcomes = my_bubble_outcomes[(start_lesson_id, final_assessment_id, path, other_path)]\n",
    "        path_pass_rate = np.mean(path_outcomes)\n",
    "        other_path_pass_rate = np.mean(other_path_outcomes)  \n",
    "        comp_sim.append(len(set(path) & set(other_path)))\n",
    "        quality_diff.append(abs(path_pass_rate - other_path_pass_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Path composition similarity')\n",
    "plt.ylabel('Path quality difference')\n",
    "plt.scatter(comp_sim, quality_diff, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
